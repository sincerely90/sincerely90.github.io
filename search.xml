<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬取学校贴吧150个帖子，统计词频，简单数据分析]]></title>
    <url>%2F2017%2F08%2F08%2Ftieba%2F</url>
    <content type="text"><![CDATA[爬取学校贴吧150个帖子，统计词频，简单数据分析一、数据采集目标站点：百度贴吧123456789101112131415161718192021222324import requestsfrom lxml import etreeimport reheaders = &#123; "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36",&#125;detail = []def titleAndurls(url): html = requests.get(url,headers=headers) urls = re.findall(r'href="(/p/.+?)"', html.text) for url in urls: getDetail('http://tieba.baidu.com'+url)def getDetail(url): html = requests.get(url, headers=headers) firstFloor = re.findall(r'class="d_post_content j_d_post_content clearfix"&gt;(.*?)&lt;',html.text,re.S) detail.append(firstFloor) print(firstFloor[0].strip()) # with open('question.txt','a+',encoding='utf-8') as f: # f.write(firstFloor.strip()+'\n')if __name__ == "__main__": urls = ['http://tieba.baidu.com/f?kw=%E5%8D%97%E9%98%B3%E7%90%86%E5%B7%A5%E5%AD%A6%E9%99%A2&amp;ie=utf-8&amp;pn=&#123;&#125;'.format(i*50) for i in range(1,4)] for url in urls: titleAndurls(url) 床多大啊，，，，谁知道 请问是不是住上铺，下面那个两联柜子可以自己使用?柜子一般怎么分配? 我是男的，不要把我当成学姐了 听说不爆照不会被罩？楼下放图 今年初三毕业，想上理工3+2，怎么报名？ 半年后终于可以对着南阳理工说句等我，风里雨里，我都想去找你——南理，虽然分数不能保证我能去，可我想冒险，就为了自己的梦，随便艾特一下之前认识的小伙伴，不知道你们还记得我么 通知书怎么查 有大神发下链接没 有没有福建龙岩的新生昂？或者福建的龙岩的老乡群，可以一起去学校噢。 南阳理工学院学生会全家福镇楼！ 想必多数男生和少数女生都在玩所谓的农药了。那么有趣的事就来了，你是喜欢哪个英雄呢，可能厉害的人会说没有不厉害的英雄，只有笨的玩者，也确实，不过对于不同等级的玩家来说，英雄的选择还是很重要的，这决定了他们的上手的容易程度。 学长学姐问一下学校附近的银行都哪些？ 学哥学姐们，学院有没有美术专科啊，不知道能不能上 请问播音系的在哪号楼上课啊？？ 中外合作的专业可以申请贫困资助吗 专升本的，用军训吗 想请问学长学姐我的通知书怎么还没到，都是南阳的应该很快就到了的呀 收到录取通知书了，一堆手机卡. 联通手机卡为什么激活不了 关于新一届的学弟学妹们，你们现在就像我当初来南工是一样的，太多的问题，食堂，环境，交通，宿舍，等等，一系列的问题，因为自己没有办法到学校看看，所以，有什么问题就问我吧，，，等待你们的骚扰 让我长高吧长高吧！！！！！ 2017年新生 本来是工商管理专业想转张仲景 现在可以转吗 求学长学姐帮助 各位学长学姐好。我是一位2017届的播音系的新生，我想了解一下这个专业在学校怎么样？宿舍条件怎么样，☺☺☺ 南阳理工学院校园分布图 请问本校宿舍怎么样 有图吗 通知书已经拿到了，打算坐火车去，学哥学姐能告诉我到了南阳火车站怎么走吗？听说有接待在哪？什么时间，怎么找啊？ 1.虽然大一学习任务不是很重，但却很容易挂科，不要老是听别的学长学姐说不难，不好好学，只管玩。在大一可以多参加一些组织社团等，同时每天抽出一点时间，哪怕半个小时来学习，你都很棒了，千万不要耽误学习，平时作业一定要从大一开始养成自己做的习惯，就算错的也没事，每次作业都自己做一遍了之后，你在考试时就会发现很轻松了。 请问一下学长学姐 想 换手 机 没有资 金 这都不是事啊！！ 喏~你们的通知书 学校西北老校区分为女生宿舍楼7~10号楼；男生宿舍楼为1~6号楼！！各位录取了的小萌新们一定要看好自己的校区分配哦！！ 17新生 国际经济与贸易 求队友 南阳理工学院 为什么又叫小清华 到现在还查不到通知书，好慌。。。。。二本的。。。。 大一毕业的话在南阳理工转换专业难吗？我被通信工程录取了，感觉好难就业啊！ 宿舍最早什么时候开门 ：与传销的“零距离”接触 求解，咱们大学中外合作办学的专业能转其他学费正常的专业么 前排求问专升本用不用军训？前排求问宿舍床铺的长和宽。 专升本升到南阳理工在哪个校区呢 本省本科二批通知书什么时候能到 有安阳的小伙伴吗😳，快来这里集合！ 学长学姐，南理寒暑假会不会组织社会实践？主要是哪一种类型的？ 大学是不是像别人说的一样，每天两节课。上课听不听随意。生活很自由啊。 南阳理工学院17届的学弟学妹们有什么对学校的问题都可以问我哦，我是一枚生活在南阳的理工小姐姐 万能的吧友啊。告诉我最早什么时候宿舍开门呀 盗图可耻 有木有扶沟高中的小伙伴 已录取，小树莓一枚，求带。 我在南工等你#哪里查专业# 谁要江西老乡群，求告知😄 广东的通知书发出没有？？ 学长学姐们我想问下征兵网的那个必须弄吗？一点也不懂被窝弄错了还能改吗？ 有玩毒奶粉的小伙伴没？河南六 跨七的 有国医专升本群吗？ 新生报到，汉语言文学，多多关照 本帖为各商家提供一个发布租房兼职招聘信息的平台，也为各位有意的同学提供一个查看此类信息的途径。 学姐学长们！想知道师范什么时候能搬到新校区！！谢谢了🙏🙏 来南工的广东学妹学弟在哪里 有没有小伙伴要处理电车的，有的话请私我 各位17的小宝贝们，你们都是哪里的呢？小学姐是哈尔滨哒，嘿嘿。河南南阳距离黑龙江坐火车要历时34小时，从此故乡只有冬夏、再无春秋…如果小伙伴有需要的话，可以随时问我哦，下附QQ 光秃秃 初中生能上南阳理工学院吗 请问各位学姐（重点），开学第一个月一共带大概多少够用啊，还有后面一个月大概多少够用？求科普！要是有播音专业的学姐来解答就更好了 福建的超提档线二十多分，能不能被征求呀 有福建的南工人嘛，本人17新生自动化专业。 女生那些不为人知的小动作 似乎是两车相向而行，发生轻碰，发生激烈争斗，几人缠斗在一起，之后在行人帮助之下，让两帮分开，凯迪拉克夫妇坐车之后，猛加油门撞一下对方家用小车而后潇洒离去，起因尚且不知，争斗结束后方有安保人员缓缓而来，询问一句，无人答复，此事结束 要生了要生了~~~ 学校里有没有专升本的班，什么时候开始啊？ 一年啦 想知道理工学院好不好 万能的小哥哥小姐姐们啊这可能是你们未来的小学妹 大一新生想找份在校兼职 求学姐学长给个介绍 谢谢哈 为啥官网不录 吉林招生办录了 我到底考没考上 可爱的萌新小石器们首先我代表理工吧务组热烈祝贺你们金榜题名！被我大理工录取！ 南阳理工学校师范是独立学院是什么意思啊，学长学姐求解答 各位学长学姐，学校里有艺术专业吗？美术的 如果你在犹豫你要加入什么社团，请来【大学生职业发展协会】 家在外省去一次实际个小时，半夜到多不安全！加入有同学在新疆那呢？长途跋涉就为了一个通知书？学生处电话打了好几个没人接！也真是气死了，帮忙吧这帖子顶上去，让校长看见！ 本人2011年本科毕业，12年底手头2万加上借来的3万，决定留学日本。13年4月赴日到东京。 想看到步步生莲的汉唐舞蹈么？ 期待你的加入👏 南阳理工学院（非中外合办）大家高考都多少分啊？ 想咨询一下，中外合作办学的学费后两年还涨吗？是必须出国吗？ 捉鸡 有食品科学与工程的小伙伴嘛!!!! 有没有专升本补习班？ 我来了哦。 南阳理工学院校图书馆读者协会 经管学长答疑～欢迎新生提问各种问题！ 有福建的新生吗？ 学校这大图灰溜溜的 我就是南阳本地的，想知道今年二本三本合并了，那成绩低的是在本校还是在医专？ 九号飞机南阳到北京有没有 北京到内蒙赤峰的可以考虑拼车了 那年今日【北京奥运，我们的骄傲！】2008年的今天，第29届 奥林匹克运动会开幕式在北京鸟巢举行。光环照亮古老的日晷， 2008名演员击缶而歌，中国画长卷缓缓打开，李宁点燃主火炬…… 世界为之惊艳！还记得当年的热血沸腾吗？2022年，相约北京-张家口冬季奥运会！期待，祝福！ 戴olc眼镜，奔光明前程! 柘城的亲们，其实咱们应该建个群。。。。。提议。。 学校有没有健身房，轮滑社呀 滴、你酷学长报道 有没有17届数字媒体艺术设计的小伙伴+ 小石器 河南省周口市鹿邑县👌 新生自助报道系统进不去网页，怎么办。 谁知道府衙发生了什么，居然小吃街都关门了 开学就要大三了，好快。。时光荏苒，人生如梦 南阳理工学院读者协会介绍NO.2之部门职责介绍 远方除了远一无所有 本帖为“广告招聘寻租服务”信息长期专用贴， 有木有，专升本，护理，河南濮阳的新生 怎么办啊？这个必须要有吧 新人求问有独立卫浴吗 弱弱的问一句496可以进贵校吗， 来自新疆的一只 小伙伴被你们学校软件工程专业录取了 有没有新生群拉一下 有新疆艺术系的伙伴吗？ 有没有市场营销（互联网营销方向）的新生啊，找不到组织(Ｔ▽Ｔ) 心好虚 学校咨询电话打不通，没有银行卡是肿么回事？？ 有没有喜欢权利的游戏的同学 被南阳理工录取了，可是不知道坐什么车到校报道比较顺利、方便，有没有辽宁的师兄、师姐能告诉一下，详细点，万分感激中！ 有中外办学专业的师哥，师姐吗？中外合办专业学校除了学费贵以外，其它别的住宿舍费伙食费……和其他专业都一样吗 我考了425，文科工商管理，可是刚才南工微信官网上的最低分比我高那么多，怎么回事啊，好吓人啊， 自动化哪个校区呐，床是多大的呢？ 请问一下本科二批录取通知书什么时候回开始寄啊？谢谢 由吉林的报B段吗 请问申请了生源地助学贷款的学生是否还需要在开学之前交学费和住宿费？？？ 机制的新生在哪个校区啊。 如题 专升本，法学，有漯河的没有？求认识 社团多不多啊 南阳理工学院国旗护卫队成立于2002年，是一个承载神圣使命、拥有巨大殊荣的特殊团体。是由团委直接领导的准军事化组织，是学校开展爱国主义教育的主阵地，以护卫国旗为使命，以弘扬爱国主义精神为己任，力求展现当代大学生团结进取、积极向上的精神风貌。以爱护国旗、贯彻国旗法为宗旨，在广大师生中营造爱国旗、护国旗的高尚的爱国主义风貌。秉承“严谨务实，率先垂范”的队训，这支充满活力的队伍为南阳理工学院的精神文明建设做出了卓越贡献。 天太热，弄点冰块消消暑 我这是已被收了吗 学弟学妹们 还没开学 先看看漂亮的南工景色吧 带你飞上天俯视一**retty南工 是飞上天的那种看咦 想飞的尽管留WeChat or Q or 私下骚扰我 你是不会后悔的 学校查寝可严嘛?晚上出去玩也不行? 图书馆借书最好不要用机器还 有可能书被吞了显示未归还 然后就等着陪书吧 然后就没然后了 又是一年开学季 【2017南阳理工学院炫域动漫社迎新宣传帖】 17届新生，美术类，视觉传达设计专业，求队友 17届，“南工”有山西的吗？求伴 二、分词统计词频(jieba)123456789101112import jieba.analysef = open('question.txt','r' ,encoding='utf-8')content = f.read()data = &#123;&#125;try: jieba.analyse.set_stop_words('stop.txt') words = jieba.analyse.extract_tags(content, topK=100, withWeight=True) for item in words: line = item[0]+'\t'+str(int(item[1]*1000)) data[item[0]] = int(item[1]*1000)finally: f.close() 三、导入pandas和pyecharts进行数据分析处理和绘图12import pandas as pdfrom pyecharts import Bar, Funnel, Radar, WordCloud 1data.keys() dict_keys([&apos;招聘&apos;, &apos;查寝&apos;, &apos;柘城&apos;, &apos;时光荏苒&apos;, &apos;全家福&apos;, &apos;老乡&apos;, &apos;欢迎&apos;, &apos;校区&apos;, &apos;师范&apos;, &apos;ID&apos;, &apos;二本&apos;, &apos;严嘛&apos;, &apos;南工&apos;, &apos;本科毕业&apos;, &apos;轮滑&apos;, &apos;准考证&apos;, &apos;理工学院&apos;, &apos;小伙伴&apos;, &apos;小可爱&apos;, &apos;拼车&apos;, &apos;专升本&apos;, &apos;介绍&apos;, &apos;国旗&apos;, &apos;学费&apos;, &apos;新生&apos;, &apos;食品科学&apos;, &apos;预科班&apos;, &apos;学生会&apos;, &apos;舞蹈&apos;, &apos;学院&apos;, &apos;报名&apos;, &apos;国医&apos;, &apos;比较顺利&apos;, &apos;学长&apos;, &apos;媒体&apos;, &apos;专业&apos;, &apos;炸鸡&apos;, &apos;美术&apos;, &apos;小萌&apos;, &apos;爱国主义&apos;, &apos;WeChat&apos;, &apos;通知书&apos;, &apos;校园&apos;, &apos;小姐姐&apos;, &apos;协会&apos;, &apos;听歌&apos;, &apos;学姐&apos;, &apos;小吃街&apos;, &apos;学校&apos;, &apos;学哥&apos;, &apos;图书馆&apos;, &apos;中外&apos;, &apos;国旗护卫队&apos;, &apos;万分感激&apos;, &apos;成绩单&apos;, &apos;大学生&apos;, &apos;17&apos;, &apos;校会君&apos;, &apos;跳街舞&apos;, &apos;吉林&apos;, &apos;宿舍&apos;, &apos;南阳&apos;, &apos;银行卡&apos;, &apos;烫头发&apos;, &apos;新生报&apos;, &apos;社团&apos;, &apos;同学&apos;, &apos;新疆&apos;, &apos;求带&apos;, &apos;校团委&apos;, &apos;萌新&apos;, &apos;捉鸡&apos;, &apos;北京&apos;, &apos;兼职&apos;, &apos;汉语言&apos;, &apos;读者&apos;, &apos;多多关照&apos;, &apos;学生处&apos;, &apos;小宝贝&apos;, &apos;艺术设计&apos;, &apos;提档线&apos;, &apos;期待&apos;, &apos;树莓&apos;, &apos;宣传机构&apos;, &apos;广东&apos;, &apos;学弟&apos;, &apos;报道&apos;, &apos;河南&apos;, &apos;freestyle&apos;, &apos;福建&apos;, &apos;组织&apos;, &apos;龙岩&apos;, &apos;开学&apos;, &apos;师姐&apos;, &apos;自动化&apos;, &apos;理工&apos;, &apos;成绩&apos;, &apos;工商管理&apos;, &apos;视觉&apos;, &apos;学妹&apos;]) 1data1 = &#123;'focus':[value for value in data.keys()],'times':[times for times in data.values()]&#125; 1df = pd.DataFrame(data1) 1df 1bar = Bar("南工贴吧帖子标题词频统计", "by:@Loading_miracle") 12bar.add('关注点',df.sort_values(by=['times'],ascending=False)[:10]['focus'].values.tolist(), df.sort_values(by=['times'],ascending=False)[:10]['times'].values.tolist()) 1bar 词云图1name = df.sort_values(by=['times'],ascending=False)['focus'].values.tolist() 1value = df.sort_values(by=['times'],ascending=False)['times'].values.tolist() 12wordcloud = WordCloud(width=1300, height=620)wordcloud.add("", name, value, word_size_range=[20, 100],shape='diamond') 1wordcloud 使用在线生成工具TAGUL生成词云图 1python3.5.3]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>jieba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锋芒工作室2017迎新]]></title>
    <url>%2F2017%2F07%2F29%2F%E9%94%8B%E8%8A%92%2F</url>
    <content type="text"><![CDATA[一、锋芒工作室简介 简介：锋芒工作室成立于2013年，前身是于2011年成立的锋芒团队。锋芒工作室自成立以来，在移动教研室张政老师的带领下，以“敢想，敢做，分享，创新”的发展宗旨带领着一届又一届的莘莘学子。社团成员团结协作，和谐相处，积极参加各项比赛活动，共同发展进步，在各个方面都取得了不错的成绩、历届学长现就职于航天科技集团第803研究所丶IBM等知名大公司。 锋芒口号：“锋芒至此，谁能挡我”，寓意：锋芒一往直前，追求目标不会因为任何困难而受到阻碍。 主要学习及研究方向：云计算、大数据、数据分析、web前端、嵌入式 二、专业介绍 1）云计算和大数据 这两个可以放在一块来说，主要是很多人都会对这两个概念有误解而且也会把它们混起来说，分别做一句话直白解释就是：云计算就是硬件资源的虚拟化;大数据就是海量数据的高效处理。如果在仔细的分析的话就是：云计算相当于我们的计算机和操作系统，将大量的硬件资源虚拟化之后再进行分配使用，在云计算领域目前的老大应该算是Amazon，可以说为云计算提供了商业化的标准，另外值得关注的还有VMware(其实从这一点可以帮助你理解云计算和虚拟化的关系)，开源的云平台最有活力的就是Openstack了;大数据相当于海量数据的“数据库”，而且通观大数据领域的发展也能看出，当前的大数据处理一直在向着近似于传统数据库体验的方向发展，Hadoop的产生使我们能够用普通机器建立稳定的处理TB级数据的集群，把传统而昂贵的并行计算等概念一下就拉到了我们的面前。整体来看的话，未来的趋势是，云计算作为计算资源的底层，支撑着上层的大数据处理，而大数据的发展趋势是，实时交互式的查询效率和分析能力，借用Google一篇技术论文中的话，“动一下鼠标就可以在秒级操作PB级别的数据”难道不让人兴奋吗? （简单科普一下：1KB=1024B 1MB=1024KB 1GB=1024MB 1TB=1024GB 1PB=1024TB 1EB=1024PB 1ZB=1024EB 1YB=1024ZB）注意之前提到的秒级实现PB级的计算操作，PB级数据何其庞大。 2）数据分析（两个案例均为学习实践）数据分析是指用适当的统计分析方法对收集来的大量数据进行分析，提取有用信息和形成结论而对数据加以详细研究和概括总结的过程。这一过程也是质量管理体系的支持过程。在实用中，数据分析可帮助人们作出判断，以便采取适当行动。其实，数据分析就在我们的生活当中，大家都使用过的Excel工具就是常用的工具之一，可以实现基本的分析工作。随着长时间的学习，接触到的工具和方法会越来越多，可以看一下我在学习编程语言python时对python的两个职位进行的一个简单分析，数据来源拉钩网（招聘网站）地址：http://www.jianshu.com/p/e7e6ef21a79f 还有我们采集了不同编程语言相关的职位进行的简单的处理分析,还对职位、薪资进行了比较，有兴趣可以看一下：http://139.199.172.91/software/language 3）web前端Web前端开发工程师，主要职责是利用(X)HTML/CSS/JavaScript/Flash等各种Web技术进行客户端产品的开发。完成客户端程序（也就是浏览器端）的开发，开发JavaScript以及Flash模块，同时结合后台开发技术模拟整体效果，进行丰富互联网的Web开发，致力于通过技术改善用户体验。大家平时在浏览网页时，是不是都感觉人家网站的页面设计挺洋气，其实，只要你认真学习，你整出来的可以比他们那些更洋气之前的数据分析展示的web用到就有前端知识，不知道大家有没有过学校的校园导航（进入学校官网便可看到）地址：校园导航这个版本是之前的老学长他们做的，现在有的路线上边可能没有，我们今年又整理新版校园导航更新了路线（初次打开可能比较慢）：新版导航 4）嵌入式IEEE（Institute of Electrical and Electronics Engineers，美国电气和电子工程师协会）对嵌入式系统的定义：“用于控制、监视或者辅助操作机器和设备的装置”嵌入式系统是一种专用的计算机系统，作为装置或设备的一部分。通常，嵌入式系统是一个控制程序存储在ROM中的嵌入式处理器控制板。事实上，所有带有数字接口的设备，如手表、微波炉、录像机、汽车等，都使用嵌入式系统，有些嵌入式系统还包含操作系统，但大多数嵌入式系统都是由单个程序实现整个控制逻辑。从应用对象上加以定义，嵌入式系统是软件和硬件的综合体，还可以涵盖机械等附属装置。国内普遍认同的嵌入式系统定义为：以应用为中心，以计算机技术为基础，软硬件可裁剪，适应应用系统对功能、可靠性、成本、体积、功耗等严格要求的专用计算机系统。前几届学长自主学习研发智能小汽车，拐杖导盲犬 三、社团活动每一年社团都会组织很多有趣的活动，下面我就简单列举几个： 1.新生培训 2.冬至包饺子 3.春季烧烤 4.各个社团的间的交流以及编程竞赛 总之，你能想到的我们都会尽可能去组织，丰富社员的大学生活。 四、你的大学你们可能会问为什么标题命名为你的大学，其实就是这样，这个就是你自己的大学，属于我们自己的，需要我们自己来规划和安排，如果你想过一个愉快而又充实的大学生活的话，那么就来锋芒吧，你还在犹豫什么呢？ 五、联系我们 群连接：点击链接加入群【锋芒工作室2017迎新群】：https://jq.qq.com/?_wv=1027&amp;k=4EiNJyS我的QQ：946439674QQ群：663280466]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫—简书首页数据抓取]]></title>
    <url>%2F2017%2F06%2F17%2F%E7%AE%80%E4%B9%A6%E9%A6%96%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[本该昨天完成的文章，拖了一天。可能是没休息好吧，昨天的在思路以及代码处理上存在很多问题，废话不多说，我们一起来看一下简书首页数据的抓取。 抓取的信息 2.2）简书首页文章信息 http://www.jianshu.com/包括：标题，作者，发表时间，阅读量，评论数，点赞数，打赏数，所投专题 单页数据的获取我们先简单看一下单页数据的抓取，所谓单页就是我们最少能获取到的数据，那么我们就先去看一下这些信息的的加载方式 通过工具我们可以看到有一个请求连接，接着我们去看一下数据 这些信息跟我们要抓取的没任何关系，那我们就可以直接从源码中找这些信息了 通过分析我们看到每一个li标签包含我们要抓取的所有信息的信息，那就可以以这个为循环点，解析第一个页面（xpath，或者通过Beautifulsoup），这里我选择的是xpath，我遇到了一个问题，就是评论数和阅读量通过xpath抓不到（可能是路径问题），我是通过正则去获取了这两个信息，下面给部分单页信息获取源码1234567891011121314151617181920212223242526272829def getData(self,url): print url html = requests.get(url,headers = self.headers,cookies = self.cookies).text response = etree.HTML(html) item = &#123;&#125; flag = 0 read = re.findall(r&apos;ic-list-read&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) comment = re.findall(r&apos;ic-list-comments&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) result = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li/div&apos;) for one in result: item[1] = one.xpath(&apos;a/text()&apos;)[0] item[2] = one.xpath(&apos;div[1]/div/a/text()&apos;)[0] item[3] = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;)[0] item[4] = read[flag] try: item[5] = comment[flag] except: item[5] = u&apos;&apos; item[6] = one.xpath(&apos;div[2]/span/text()&apos;)[0].strip() try: item[7] = one.xpath(&apos;div[2]/span[2]/text()&apos;)[0].strip() except: item[7] = u&apos;0&apos; try: item[8] = one.xpath(&apos;div[2]/a[1]/text()&apos;)[0] except: item[8] = u&apos;&apos; flag += 1 row = [item[i] for i in range(1, 9)] 1-8分别对应之前提到的 标题，作者，发表时间，阅读量，评论数，点赞数，打赏数，所投专题 网页加载方式及分页问题我们在首页滑动鼠标会发现，信息越来越多，但是还有一点就是可以看到URL并没有发生变化，所以其他页面的信息就是异步加载了，那么接着就要去抓包了 通过滑动鼠标，一段时间后我们可以看到，这个连接，但其实它已经请求了很多页面了 这个时候我们看到，其实有两个异步请求，已经悄悄在页面中填充了数据那么我们点击阅读更多又会有什么变化呢？ 那么我们是不是可以通过，改变页码（page）去完成分页呢？因为首页和七日热门比较类似，按照我之前爬取七日热门的思路去抓取（spiders——简书7日热门（scrapy）），但是显然精简过得URL不能抓取首页的信息，那么没办法就只能把全部的参数都怼进去试试了，首先我们来看一下除了page这个参数之外，seen_snote_ids[]参数应该在那找 我们看到第一页并没有带参数， 我们再去看一下第二页的请求信息 有很多id，那么我们应该去哪找呢，我们先去看一下第一页的源码 看到这些数字，是不是和第二页的参数有关系呢，经过对比确实和第二页的参数id一致，有了头绪我们再去看一下第三页的（进一步确定携带的参数） 经过分析，我们很巧的发现第三页的参数是40个第二页是20个，第一个0个，并且，第二页的id参数，我们可以在第一页源码中拿到，那第三页的是不是也可以在第二页中看到呢？，我们去看一下第二页源码 因为网页就是直接加载的，我们大概确定一下第二页的位置，然后对比去对比第三页的部分参数信息 大家如果仔细去对比是可以发现，确实第三页的参数包含了第一个页面和第二个页面中的id信息。现在差不多我们对这个网页的加载方式，以及分页方式有了进一步的理解，就是之后的每一页除了page参数改变之外，携带的seen_snote_ids[]是上（几）页的所有id参数，那么这个到底有多少页呢，我真的去不断点击加载最终，page参数停留在了15页（seen_snote_ids[]的数量看更是非常大），并且也没有出现阅读更多字样，我们来看一下 我们可以看到请求的URL的长度，参数一直在增加，所以我暂且就认为i这个是15页，下边给一下获取id以及分页URL的构造示例代码：1.获取id12345html = requests.get(url,headers = self.headers,cookies = self.cookies).textresponse = etree.HTML(html)ids = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li&apos;)for one in ids: one = &apos;seen_snote_ids[]=&apos; + one.xpath(&apos;@data-note-id&apos;)[0] 2.构造页码12345def totalPage(self): for i in range(1,16): data = &apos;&amp;&apos;.join(self.params) url = &apos;http://www.jianshu.com/?&apos; + data + &apos;&amp;page=&#123;&#125;&apos;.format(i) self.getData(url) 遇到的问题+样例源码1.遇到的问题之前按照我简书七日热门的思路去写，最后获取到的都是重复数据，并且在添加id之后也是重复数据，罗罗攀 给我看了向右奔跑老大之前关于首页分析的文章，看了之后和我的对比，感觉差不多，但是我就是出不来数据，之后各位老哥们就说可能是参数不够吧，LEONYao老哥还说可以把参数都怼进去，满状态轰炸，向右奔跑老大之后说带个cookies可行，测试之后真的可行（一个小小的cookies困扰了很长时间，没想起来带cookies）2.示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# -*- coding:utf-8 -*-from lxml import etreeimport requestsimport refrom Class.store_csv import CSVclass Spider(object): headers = &#123; &quot;user-agent&quot;: &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&quot; &#125; cookies = &#123; &apos;UM_distinctid&apos;: &apos;15ac11bdff316-0ce09a4511f531-67f1a39-100200-15ac11bdff447d&apos;, &apos;CNZZDATA1258679142&apos;: &apos;1034687255-1492307094-%7C1493259066&apos;, &apos;remember_user_token&apos;: &apos;W1s1MjA4MDY0XSwiJDJhJDEwJFVWVjUwbXBsS1hldkc1d0l3UG5DSmUiLCIxNDk0ODkyNTg0LjczNDM2ODgiXQ%3D%3D--f04b34c274980b45e5f7ee17c2686aeb4b567197&apos;, &apos;_gat&apos;: &apos;1&apos;, &apos;_session_id&apos;: &apos;N0tvclN3V09wZ25UNFloZ0NrRTBVT3ZYQUR5VkRlV1c2Tno1bnNZc3dmQm9kQ3hmOGY4a0dFUlVLMDdPYWZJdCsydGJMaENZVU1XSHdZMHozblNhUERqaldYTHNWYXVPd2tISHVCeWJtbUFwMjJxQ3lyU2NZaTNoVUZsblV4Si94N2hRRC94MkJkUjhGNkNCYm1zVmM0R0ZqR2hFSFltZnhEcXVLbG54SlNSQU5lR0dtZ2MxOWlyYWVBMVl1a1lMVkFTYS8yQVF3bGFiR2hMblcweTU5cnR5ZTluTGlZdnFKbUdFWUYzcm9sZFZLOGduWFdnUU9yN3I0OTNZbWMxQ2UvbU5aQnByQmVoMFNjR1NmaDJJSXF6WHBYQXpPQnBVRVJnaVZVQ2xUR1p4MXNUaDhQSE80N1paLzg0amlBdjRxMU15a0JORlB1YXJ4V2g0b3hYZXpjR1NkSHVVdnA2RkgvVkJmdkJzdTg5ODhnUVRCSnN2cnlwRVJvWWc4N0lZMWhCMWNSMktMMWNERktycE0wcHFhTnYyK3ZoSWFSUFQzbkVyMDlXd2d5bz0tLThrdXQ2cFdRTTNaYXFRZm5RNWtYZUE9PQ%3D%3D--bc52e90a4f1d720f4766a5894866b3764c0482dd&apos;, &apos;_ga&apos;: &apos;GA1.2.1781682389.1492310343&apos;, &apos;_gid&apos;: &apos;GA1.2.163793537.1495583991&apos;, &apos;Hm_lvt_0c0e9d9b1e7d617b3e6842e85b9fb068&apos;: &apos;1495360310,1495416048,1495516194,1495583956&apos;, &apos;Hm_lpvt_0c0e9d9b1e7d617b3e6842e85b9fb068&apos;: &apos;1495583991&apos; &#125; params = [] def __init__(self): field = [&apos;标题&apos;, &apos;作者&apos;, &apos;发表时间&apos;, &apos;阅读量&apos;, &apos;评论数&apos;, &apos;点赞数&apos;, &apos;打赏数&apos;, &apos;所投专题&apos;] self.write = CSV(&apos;main.csv&apos;, field) def totalPage(self): for i in range(1,16): data = &apos;&amp;&apos;.join(self.params) url = &apos;http://www.jianshu.com/?&apos; + data + &apos;&amp;page=&#123;&#125;&apos;.format(i) self.getData(url) def getData(self,url): print url html = requests.get(url,headers = self.headers,cookies = self.cookies).text response = etree.HTML(html) ids = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li&apos;) for one in ids: one = &apos;seen_snote_ids[]=&apos; + one.xpath(&apos;@data-note-id&apos;)[0] self.params.append(one) item = &#123;&#125; flag = 0 read = re.findall(r&apos;ic-list-read&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) comment = re.findall(r&apos;ic-list-comments&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) result = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li/div&apos;) for one in result: item[1] = one.xpath(&apos;a/text()&apos;)[0] item[2] = one.xpath(&apos;div[1]/div/a/text()&apos;)[0] item[3] = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;)[0] item[4] = read[flag] try: item[5] = comment[flag] except: item[5] = u&apos;&apos; item[6] = one.xpath(&apos;div[2]/span/text()&apos;)[0].strip() try: item[7] = one.xpath(&apos;div[2]/span[2]/text()&apos;)[0].strip() except: item[7] = u&apos;0&apos; try: item[8] = one.xpath(&apos;div[2]/a[1]/text()&apos;)[0] except: item[8] = u&apos;&apos; flag += 1 row = [item[i] for i in range(1, 9)] self.write.writeRow(row)if __name__ == &quot;__main__&quot;: jian = Spider() jian.totalPage() 结果截图 总结现在想来，在爬取网站时，我们可以携带尽可能全的参数（俗话说，礼多人不怪），避免遇到我这个错误，scrapy版本正在写，有兴趣的可以私聊参考源码。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版万年历]]></title>
    <url>%2F2017%2F06%2F15%2F%E4%B8%87%E5%B9%B4%E5%8E%86%2F</url>
    <content type="text"><![CDATA[万年历这个题目几乎是不论学哪种编程语言必要尝试的一个小知识，综合了循环，逻辑关系判断等各编程语言的基础知识。今天我们一起用Python实现简单的万年历功能（查看某年各个月份日历和查看确定月份日历）。网上大概浏览了一部分代码，发现实现方法都是大同小异，本篇文章代码可能会略有不同，可供参考。记得之前在学C语言的时候就去写过万年历，现在已经忘得一干二净了，用Python实现万年历主要参考了两篇文章，第一篇是向右奔跑老大之前写的java版万年历,还有一篇是在网上找的C语言实现万年历,大家也可以去参考这两篇文章的方法。 难点万年历实现的难点自我感觉有以下几个： 1.计算某年一月一日为周几 2.计算某年确定月份的第一天为周几 3.整个日历格式的设定（调试是真的烦） 万年历实现1.计算某年一月一日为周几这里我看到网上大部分都是写的1990年一月一日为周一这个点进行判断计算其他年份的一月一日，或者是确定月份的第一天。还有一种是计算从1——年份-1这一段时间的总天数，然后+1除以7去计算得到该年份的一月一日或者是确定月份的第一天为周几（这个应该是一种计算问题，具体更多实现方法可以自己去查），这里我采用的是计算从1——该年的上一年的总天数+1，计算该年份的一月一日的周几。2.计算某年确定月份的第一天为周几 同样的道理，计算总天数之后加上今年已经过去的月份的总天数再加1除以7取余得到3.格式问题这个没办法，只能自己去调试，调整到一个合适的输出格式即可 文字表述可能不清楚，可以参考代码+注释 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*-coding:utf-8 -*-# @name ：Python万年历# @author ：loading_miracleclass Calendar(object): days = [31,28,31,30,31,30,31,31,30,31,30,31] def __init__(self, year=2017):#初始化默认2017年 self.year = year if self.yearDays(year) == 366: self.days[1] = 29 self.totalDays = 0 for i in range(1, self.year): self.totalDays += self.yearDays(i) #判断每年的天数 def yearDays(self,year): return 366 if (year % 4 == 0 and year % 100 != 0) or year % 400 == 0 else 365 # 查看某个月的日历 def months(self,month): totalDays = self.totalDays for i in range(1,month): totalDays+=self.days[i-1] #计算确定月份的第一天为周几 self.week = (totalDays+1) % 7 self.show(month) #查看全年日历 def wholeYear(self): # 计算确定年份的一月一日为周几 self.week = (self.totalDays+1) % 7 for i in range(1,13): self.show(i) #显示输出函数 def show(self,month): print &apos;\t\t&#123;&#125;年&#123;&#125;月份日历&apos;.format(self.year,month) print &apos;Sun Mon Tues Web Thur Fri Sat&apos; print &apos;-----------------------------------------&apos; begin = 1 for j in range(0, self.week): print &apos;%4s&apos; % &apos;&apos;, while begin &lt;= self.days[month - 1]: print &apos;%4d&apos; % begin, begin += 1 self.week = (self.week + 1) % 7 if self.week % 7 == 0: print print &apos;\n\n&apos;if __name__ == &quot;__main__&quot;: data = Calendar() data.wholeYear() data.months(5) 效果截图]]></content>
      <categories>
        <category>Python编程练习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>基础编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫—简书七日热门（scrapy）]]></title>
    <url>%2F2017%2F06%2F14%2FPython%E7%88%AC%E8%99%AB%E2%80%94%E7%AE%80%E4%B9%A6%E4%B8%83%E6%97%A5%E7%83%AD%E9%97%A8%EF%BC%88scrapy%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简书七日热门的数据，主要爬取了以下几个字段： 1.用户2.标题3.阅读量4.评论量5.获赞量6.打赏数7.文章发表时间8.被哪些专题收录 首先我们可以先看一下我们要获取字段的位置 框中的其他数据都可以通过xpath获取到，标注箭头的为我在当前页面通过xpath获取不到的或者出现问题的，不过别着急，二级页面中包含我们想要的所有数据。 从这个页面我们可以看到，在第一级页面中的数据，在这一页都可以看到，但我们只选取我们想要的字段,箭头指向的为上一级页面中通过xpath获取出现问题的字段，在当前页面获取同样出现了问题，所以果断采用正则在源码中获取 我们可以看到在源码中可以看到这些字段,那么就可以通过正则来匹配相应的字段我们可以看一下这一篇文章被哪些专题收录，在文章的最下边 现在我们整明白了所有字段的位置，接下来就开始去抓取这些字段，首先我们先分析页面的分页问题，通过观察发现页面是动态加载的，再点击更多后会请求新的数据，我们来看一下整个页面的url规则，以及获取被收录专题的数据的url规则，我这里简单列一下一些截图，具体可以参考罗罗攀的Python爬虫之简书七日热门数据爬取（异步加载详解），写的特别详细。 通过这些字段我们可以想到一种构造url的方法1234def start_requests(self): for a in range(1,6): self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a yield scrapy.Request(self.url,self.parse) 我们再来看一下被收录专题的url规则 从这两张图可以看出，URL主要变化点在id和和page，ID可以从第三张图片中通过正则从源码获取从最后一张图可以看出收录的专题数有5页专题名都存在title中所以我们就可以通过第一条URL获取总页数，然后构造其他的url。构造方法如下：1234567891011121314id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0]url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %iddatas = []result = requests.get(url)data = json.loads(result.text)for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;])count = data[&apos;total_pages&apos;]for one in range(2,count+1): url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one) result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) 通过以上方法可以成功的找到页面和数据，接下来一个很重要的问题，怎们在多级页面中统一数据，因为我们所有的数据是集中在两个页面是实现的，那么怎样才可以实现多级页面传递数据呢？完成多级页面传递参数主要是通过meta这个属性，具体传递方法可以参照向右奔跑老大的Scrapy抓取在不同级别Request之间传递参数，这篇文章有很详细的介绍。在这个地方遇到的一个坑：多级页面传递后，发现总是存的是重复的内容，在这里一定要注意，因为肯定是使用for循环去获取相关标题的URL然后进入子页面去获取其他相关字段的信息，但是如果在这个地方定义了item，然后在多级传递参数，就可能出现重复的情况，所以就采用了在第一级页面获取到的数据先用变量存储，通过meta这个属性标签带入二级页面，可以看一下处理代码：12345678910111213for one in result: user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0] title = one.xpath(&apos;a/text()&apos;).extract()[0] zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0] try: shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0] except: shang = u&apos;0&apos; publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0] link = one.xpath(&apos;a/@href&apos;).extract()[0] url = urljoin_rfc(base_url,link) yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title, &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type) 通过上边的简单介绍，应该对整个流程和规则有了一定的了解，下边直接上代码 1.item.py123456789101112import scrapyclass JianshuItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() user = scrapy.Field() #用户 title = scrapy.Field() #标题 count_read = scrapy.Field() #阅读量 ping = scrapy.Field() #评论量 count_zan = scrapy.Field() #喜欢 shang = scrapy.Field() #赞赏 publish = scrapy.Field() #时间 sp_title = scrapy.Field() #被专题书录 ##2.jianshuSpider.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#-*- coding:utf-8 -*-import scrapyfrom ..items import JianshuItemfrom scrapy.utils.url import urljoin_rfcimport requestsimport reimport jsonclass jianshuSpider(scrapy.Spider): name = &apos;jianshu&apos; def start_requests(self): for a in range(1,6): self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a yield scrapy.Request(self.url,self.parse) def parse(self, response): result = response.xpath(&apos;/html/body/div[1]/div/div[1]/div/ul/li/div&apos;) base_url = &apos;http://www.jianshu.com&apos; for one in result: user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0] title = one.xpath(&apos;a/text()&apos;).extract()[0] zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0] try: shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0] except: shang = u&apos;0&apos; publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0] link = one.xpath(&apos;a/@href&apos;).extract()[0] url = urljoin_rfc(base_url,link) yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title, &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type) def parse_type(self,response): item = JianshuItem() item[&apos;user&apos;] = response.meta[&apos;user&apos;] item[&apos;title&apos;] = response.meta[&apos;title&apos;] item[&apos;count_read&apos;] = re.findall(&apos;&quot;views_count&quot;:(.*?),&apos;,response.text,re.S)[0] item[&apos;ping&apos;] = re.findall(&apos;&quot;comments_count&quot;:(.*?),&apos;,response.text,re.S)[0] item[&apos;count_zan&apos;] = response.meta[&apos;zan&apos;] item[&apos;shang&apos;] = response.meta[&apos;shang&apos;] item[&apos;publish&apos;] = response.meta[&apos;publish&apos;] id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0] url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %id datas = [] result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) count = data[&apos;total_pages&apos;] for one in range(2,count+1): url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one) result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) try: item[&apos;sp_title&apos;] = &quot; &quot;.join(datas).encode(&apos;utf-8&apos;) except: item[&apos;sp_title&apos;] = u&apos;&apos; yield item 3.结果 总结先说一下存在的问题：在对一个页面进行爬取的时候没做一个整体的考虑，所以在不同页面之前传递的参数比较多，代码看起来有点乱，之后可以将所有的数据全部在二级页面获取。还有一点，可以看到在处理json数据那一块，我选择使用了requests库去操作，这是因为如果使用scrapy.Request去操作的话，要通过回调函数去处理，还要在进行参数的传递，造成很多麻烦，然后准备尝试去写一个用requests和beautifulsoup爬虫，进行一个对比。写的第一篇关于思想和技术方面的文章，由于时间的关系所以可能解释的不是很详细，如果存在问题可以在下方评论一起探讨。最后还是要感谢一下罗罗攀和向右奔跑的相关文章和罗罗攀大哥无私的帮助，如果有时间的话会对数据进一步处理，做一个相关分析。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F06%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>