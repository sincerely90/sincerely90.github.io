<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SVN服务的搭建（一）]]></title>
    <url>%2F2017%2F06%2F15%2F%E4%B8%87%E5%B9%B4%E5%8E%86%2F</url>
    <content type="text"><![CDATA[万年历这个题目几乎是不论学哪种编程语言必要尝试的一个小知识，综合了循环，逻辑关系判断等各编程语言的基础知识。今天我们一起用Python实现简单的万年历功能（查看某年各个月份日历和查看确定月份日历）。网上大概浏览了一部分代码，发现实现方法都是大同小异，本篇文章代码可能会略有不同，可供参考。记得之前在学C语言的时候就去写过万年历，现在已经忘得一干二净了，用Python实现万年历主要参考了两篇文章，第一篇是向右奔跑老大之前写的java版万年历,还有一篇是在网上找的C语言实现万年历,大家也可以去参考这两篇文章的方法。 难点万年历实现的难点自我感觉有以下几个： 1.计算某年一月一日为周几 2.计算某年确定月份的第一天为周几 3.整个日历格式的设定（调试是真的烦） 万年历实现1.计算某年一月一日为周几这里我看到网上大部分都是写的1990年一月一日为周一这个点进行判断计算其他年份的一月一日，或者是确定月份的第一天。还有一种是计算从1——年份-1这一段时间的总天数，然后+1除以7去计算得到该年份的一月一日或者是确定月份的第一天为周几（这个应该是一种计算问题，具体更多实现方法可以自己去查），这里我采用的是计算从1——该年的上一年的总天数+1，计算该年份的一月一日的周几。2.计算某年确定月份的第一天为周几 同样的道理，计算总天数之后加上今年已经过去的月份的总天数再加1除以7取余得到3.格式问题这个没办法，只能自己去调试，调整到一个合适的输出格式即可 文字表述可能不清楚，可以参考代码+注释 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*-coding:utf-8 -*-# @name ：Python万年历# @author ：loading_miracleclass Calendar(object): days = [31,28,31,30,31,30,31,31,30,31,30,31] def __init__(self, year=2017):#初始化默认2017年 self.year = year if self.yearDays(year) == 366: self.days[1] = 29 self.totalDays = 0 for i in range(1, self.year): self.totalDays += self.yearDays(i) #判断每年的天数 def yearDays(self,year): return 366 if (year % 4 == 0 and year % 100 != 0) or year % 400 == 0 else 365 # 查看某个月的日历 def months(self,month): totalDays = self.totalDays for i in range(1,month): totalDays+=self.days[i-1] #计算确定月份的第一天为周几 self.week = (totalDays+1) % 7 self.show(month) #查看全年日历 def wholeYear(self): # 计算确定年份的一月一日为周几 self.week = (self.totalDays+1) % 7 for i in range(1,13): self.show(i) #显示输出函数 def show(self,month): print &apos;\t\t&#123;&#125;年&#123;&#125;月份日历&apos;.format(self.year,month) print &apos;Sun Mon Tues Web Thur Fri Sat&apos; print &apos;-----------------------------------------&apos; begin = 1 for j in range(0, self.week): print &apos;%4s&apos; % &apos;&apos;, while begin &lt;= self.days[month - 1]: print &apos;%4d&apos; % begin, begin += 1 self.week = (self.week + 1) % 7 if self.week % 7 == 0: print print &apos;\n\n&apos;if __name__ == &quot;__main__&quot;: data = Calendar() data.wholeYear() data.months(5) 效果截图]]></content>
      <categories>
        <category>Python编程练习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>基础编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫—简书七日热门（scrapy）]]></title>
    <url>%2F2017%2F06%2F14%2FPython%E7%88%AC%E8%99%AB%E2%80%94%E7%AE%80%E4%B9%A6%E4%B8%83%E6%97%A5%E7%83%AD%E9%97%A8%EF%BC%88scrapy%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简书七日热门的数据，主要爬取了以下几个字段： 1.用户2.标题3.阅读量4.评论量5.获赞量6.打赏数7.文章发表时间8.被哪些专题收录 首先我们可以先看一下我们要获取字段的位置 框中的其他数据都可以通过xpath获取到，标注箭头的为我在当前页面通过xpath获取不到的或者出现问题的，不过别着急，二级页面中包含我们想要的所有数据。 从这个页面我们可以看到，在第一级页面中的数据，在这一页都可以看到，但我们只选取我们想要的字段,箭头指向的为上一级页面中通过xpath获取出现问题的字段，在当前页面获取同样出现了问题，所以果断采用正则在源码中获取 我们可以看到在源码中可以看到这些字段,那么就可以通过正则来匹配相应的字段我们可以看一下这一篇文章被哪些专题收录，在文章的最下边 现在我们整明白了所有字段的位置，接下来就开始去抓取这些字段，首先我们先分析页面的分页问题，通过观察发现页面是动态加载的，再点击更多后会请求新的数据，我们来看一下整个页面的url规则，以及获取被收录专题的数据的url规则，我这里简单列一下一些截图，具体可以参考罗罗攀的Python爬虫之简书七日热门数据爬取（异步加载详解），写的特别详细。 通过这些字段我们可以想到一种构造url的方法1234def start_requests(self): for a in range(1,6): self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a yield scrapy.Request(self.url,self.parse) 我们再来看一下被收录专题的url规则 从这两张图可以看出，URL主要变化点在id和和page，ID可以从第三张图片中通过正则从源码获取从最后一张图可以看出收录的专题数有5页专题名都存在title中所以我们就可以通过第一条URL获取总页数，然后构造其他的url。构造方法如下：1234567891011121314id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0]url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %iddatas = []result = requests.get(url)data = json.loads(result.text)for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;])count = data[&apos;total_pages&apos;]for one in range(2,count+1): url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one) result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) 通过以上方法可以成功的找到页面和数据，接下来一个很重要的问题，怎们在多级页面中统一数据，因为我们所有的数据是集中在两个页面是实现的，那么怎样才可以实现多级页面传递数据呢？完成多级页面传递参数主要是通过meta这个属性，具体传递方法可以参照向右奔跑老大的Scrapy抓取在不同级别Request之间传递参数，这篇文章有很详细的介绍。在这个地方遇到的一个坑：多级页面传递后，发现总是存的是重复的内容，在这里一定要注意，因为肯定是使用for循环去获取相关标题的URL然后进入子页面去获取其他相关字段的信息，但是如果在这个地方定义了item，然后在多级传递参数，就可能出现重复的情况，所以就采用了在第一级页面获取到的数据先用变量存储，通过meta这个属性标签带入二级页面，可以看一下处理代码：12345678910111213for one in result: user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0] title = one.xpath(&apos;a/text()&apos;).extract()[0] zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0] try: shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0] except: shang = u&apos;0&apos; publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0] link = one.xpath(&apos;a/@href&apos;).extract()[0] url = urljoin_rfc(base_url,link) yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title, &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type) 通过上边的简单介绍，应该对整个流程和规则有了一定的了解，下边直接上代码 1.item.py123456789101112import scrapyclass JianshuItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() user = scrapy.Field() #用户 title = scrapy.Field() #标题 count_read = scrapy.Field() #阅读量 ping = scrapy.Field() #评论量 count_zan = scrapy.Field() #喜欢 shang = scrapy.Field() #赞赏 publish = scrapy.Field() #时间 sp_title = scrapy.Field() #被专题书录 ##2.jianshuSpider.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#-*- coding:utf-8 -*-import scrapyfrom ..items import JianshuItemfrom scrapy.utils.url import urljoin_rfcimport requestsimport reimport jsonclass jianshuSpider(scrapy.Spider): name = &apos;jianshu&apos; def start_requests(self): for a in range(1,6): self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a yield scrapy.Request(self.url,self.parse) def parse(self, response): result = response.xpath(&apos;/html/body/div[1]/div/div[1]/div/ul/li/div&apos;) base_url = &apos;http://www.jianshu.com&apos; for one in result: user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0] title = one.xpath(&apos;a/text()&apos;).extract()[0] zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0] try: shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0] except: shang = u&apos;0&apos; publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0] link = one.xpath(&apos;a/@href&apos;).extract()[0] url = urljoin_rfc(base_url,link) yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title, &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type) def parse_type(self,response): item = JianshuItem() item[&apos;user&apos;] = response.meta[&apos;user&apos;] item[&apos;title&apos;] = response.meta[&apos;title&apos;] item[&apos;count_read&apos;] = re.findall(&apos;&quot;views_count&quot;:(.*?),&apos;,response.text,re.S)[0] item[&apos;ping&apos;] = re.findall(&apos;&quot;comments_count&quot;:(.*?),&apos;,response.text,re.S)[0] item[&apos;count_zan&apos;] = response.meta[&apos;zan&apos;] item[&apos;shang&apos;] = response.meta[&apos;shang&apos;] item[&apos;publish&apos;] = response.meta[&apos;publish&apos;] id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0] url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %id datas = [] result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) count = data[&apos;total_pages&apos;] for one in range(2,count+1): url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one) result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) try: item[&apos;sp_title&apos;] = &quot; &quot;.join(datas).encode(&apos;utf-8&apos;) except: item[&apos;sp_title&apos;] = u&apos;&apos; yield item 3.结果 总结先说一下存在的问题：在对一个页面进行爬取的时候没做一个整体的考虑，所以在不同页面之前传递的参数比较多，代码看起来有点乱，之后可以将所有的数据全部在二级页面获取。还有一点，可以看到在处理json数据那一块，我选择使用了requests库去操作，这是因为如果使用scrapy.Request去操作的话，要通过回调函数去处理，还要在进行参数的传递，造成很多麻烦，然后准备尝试去写一个用requests和beautifulsoup爬虫，进行一个对比。写的第一篇关于思想和技术方面的文章，由于时间的关系所以可能解释的不是很详细，如果存在问题可以在下方评论一起探讨。最后还是要感谢一下罗罗攀和向右奔跑的相关文章和罗罗攀大哥无私的帮助，如果有时间的话会对数据进一步处理，做一个相关分析。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F06%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>