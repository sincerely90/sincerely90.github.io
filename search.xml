<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫—简书首页数据抓取]]></title>
    <url>%2F2017%2F06%2F17%2F%E7%AE%80%E4%B9%A6%E9%A6%96%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[本该昨天完成的文章，拖了一天。可能是没休息好吧，昨天的在思路以及代码处理上存在很多问题，废话不多说，我们一起来看一下简书首页数据的抓取。 抓取的信息 2.2）简书首页文章信息 http://www.jianshu.com/包括：标题，作者，发表时间，阅读量，评论数，点赞数，打赏数，所投专题 单页数据的获取我们先简单看一下单页数据的抓取，所谓单页就是我们最少能获取到的数据，那么我们就先去看一下这些信息的的加载方式 通过工具我们可以看到有一个请求连接，接着我们去看一下数据 这些信息跟我们要抓取的没任何关系，那我们就可以直接从源码中找这些信息了 通过分析我们看到每一个li标签包含我们要抓取的所有信息的信息，那就可以以这个为循环点，解析第一个页面（xpath，或者通过Beautifulsoup），这里我选择的是xpath，我遇到了一个问题，就是评论数和阅读量通过xpath抓不到（可能是路径问题），我是通过正则去获取了这两个信息，下面给部分单页信息获取源码1234567891011121314151617181920212223242526272829def getData(self,url): print url html = requests.get(url,headers = self.headers,cookies = self.cookies).text response = etree.HTML(html) item = &#123;&#125; flag = 0 read = re.findall(r&apos;ic-list-read&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) comment = re.findall(r&apos;ic-list-comments&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) result = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li/div&apos;) for one in result: item[1] = one.xpath(&apos;a/text()&apos;)[0] item[2] = one.xpath(&apos;div[1]/div/a/text()&apos;)[0] item[3] = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;)[0] item[4] = read[flag] try: item[5] = comment[flag] except: item[5] = u&apos;&apos; item[6] = one.xpath(&apos;div[2]/span/text()&apos;)[0].strip() try: item[7] = one.xpath(&apos;div[2]/span[2]/text()&apos;)[0].strip() except: item[7] = u&apos;0&apos; try: item[8] = one.xpath(&apos;div[2]/a[1]/text()&apos;)[0] except: item[8] = u&apos;&apos; flag += 1 row = [item[i] for i in range(1, 9)] 1-8分别对应之前提到的 标题，作者，发表时间，阅读量，评论数，点赞数，打赏数，所投专题 网页加载方式及分页问题我们在首页滑动鼠标会发现，信息越来越多，但是还有一点就是可以看到URL并没有发生变化，所以其他页面的信息就是异步加载了，那么接着就要去抓包了 通过滑动鼠标，一段时间后我们可以看到，这个连接，但其实它已经请求了很多页面了 这个时候我们看到，其实有两个异步请求，已经悄悄在页面中填充了数据那么我们点击阅读更多又会有什么变化呢？ 那么我们是不是可以通过，改变页码（page）去完成分页呢？因为首页和七日热门比较类似，按照我之前爬取七日热门的思路去抓取（spiders——简书7日热门（scrapy）），但是显然精简过得URL不能抓取首页的信息，那么没办法就只能把全部的参数都怼进去试试了，首先我们来看一下除了page这个参数之外，seen_snote_ids[]参数应该在那找 我们看到第一页并没有带参数， 我们再去看一下第二页的请求信息 有很多id，那么我们应该去哪找呢，我们先去看一下第一页的源码 看到这些数字，是不是和第二页的参数有关系呢，经过对比确实和第二页的参数id一致，有了头绪我们再去看一下第三页的（进一步确定携带的参数） 经过分析，我们很巧的发现第三页的参数是40个第二页是20个，第一个0个，并且，第二页的id参数，我们可以在第一页源码中拿到，那第三页的是不是也可以在第二页中看到呢？，我们去看一下第二页源码 因为网页就是直接加载的，我们大概确定一下第二页的位置，然后对比去对比第三页的部分参数信息 大家如果仔细去对比是可以发现，确实第三页的参数包含了第一个页面和第二个页面中的id信息。现在差不多我们对这个网页的加载方式，以及分页方式有了进一步的理解，就是之后的每一页除了page参数改变之外，携带的seen_snote_ids[]是上（几）页的所有id参数，那么这个到底有多少页呢，我真的去不断点击加载最终，page参数停留在了15页（seen_snote_ids[]的数量看更是非常大），并且也没有出现阅读更多字样，我们来看一下 我们可以看到请求的URL的长度，参数一直在增加，所以我暂且就认为i这个是15页，下边给一下获取id以及分页URL的构造示例代码：1.获取id12345html = requests.get(url,headers = self.headers,cookies = self.cookies).textresponse = etree.HTML(html)ids = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li&apos;)for one in ids: one = &apos;seen_snote_ids[]=&apos; + one.xpath(&apos;@data-note-id&apos;)[0] 2.构造页码12345def totalPage(self): for i in range(1,16): data = &apos;&amp;&apos;.join(self.params) url = &apos;http://www.jianshu.com/?&apos; + data + &apos;&amp;page=&#123;&#125;&apos;.format(i) self.getData(url) 遇到的问题+样例源码1.遇到的问题之前按照我简书七日热门的思路去写，最后获取到的都是重复数据，并且在添加id之后也是重复数据，罗罗攀 给我看了向右奔跑老大之前关于首页分析的文章，看了之后和我的对比，感觉差不多，但是我就是出不来数据，之后各位老哥们就说可能是参数不够吧，LEONYao老哥还说可以把参数都怼进去，满状态轰炸，向右奔跑老大之后说带个cookies可行，测试之后真的可行（一个小小的cookies困扰了很长时间，没想起来带cookies）2.示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# -*- coding:utf-8 -*-from lxml import etreeimport requestsimport refrom Class.store_csv import CSVclass Spider(object): headers = &#123; &quot;user-agent&quot;: &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&quot; &#125; cookies = &#123; &apos;UM_distinctid&apos;: &apos;15ac11bdff316-0ce09a4511f531-67f1a39-100200-15ac11bdff447d&apos;, &apos;CNZZDATA1258679142&apos;: &apos;1034687255-1492307094-%7C1493259066&apos;, &apos;remember_user_token&apos;: &apos;W1s1MjA4MDY0XSwiJDJhJDEwJFVWVjUwbXBsS1hldkc1d0l3UG5DSmUiLCIxNDk0ODkyNTg0LjczNDM2ODgiXQ%3D%3D--f04b34c274980b45e5f7ee17c2686aeb4b567197&apos;, &apos;_gat&apos;: &apos;1&apos;, &apos;_session_id&apos;: &apos;N0tvclN3V09wZ25UNFloZ0NrRTBVT3ZYQUR5VkRlV1c2Tno1bnNZc3dmQm9kQ3hmOGY4a0dFUlVLMDdPYWZJdCsydGJMaENZVU1XSHdZMHozblNhUERqaldYTHNWYXVPd2tISHVCeWJtbUFwMjJxQ3lyU2NZaTNoVUZsblV4Si94N2hRRC94MkJkUjhGNkNCYm1zVmM0R0ZqR2hFSFltZnhEcXVLbG54SlNSQU5lR0dtZ2MxOWlyYWVBMVl1a1lMVkFTYS8yQVF3bGFiR2hMblcweTU5cnR5ZTluTGlZdnFKbUdFWUYzcm9sZFZLOGduWFdnUU9yN3I0OTNZbWMxQ2UvbU5aQnByQmVoMFNjR1NmaDJJSXF6WHBYQXpPQnBVRVJnaVZVQ2xUR1p4MXNUaDhQSE80N1paLzg0amlBdjRxMU15a0JORlB1YXJ4V2g0b3hYZXpjR1NkSHVVdnA2RkgvVkJmdkJzdTg5ODhnUVRCSnN2cnlwRVJvWWc4N0lZMWhCMWNSMktMMWNERktycE0wcHFhTnYyK3ZoSWFSUFQzbkVyMDlXd2d5bz0tLThrdXQ2cFdRTTNaYXFRZm5RNWtYZUE9PQ%3D%3D--bc52e90a4f1d720f4766a5894866b3764c0482dd&apos;, &apos;_ga&apos;: &apos;GA1.2.1781682389.1492310343&apos;, &apos;_gid&apos;: &apos;GA1.2.163793537.1495583991&apos;, &apos;Hm_lvt_0c0e9d9b1e7d617b3e6842e85b9fb068&apos;: &apos;1495360310,1495416048,1495516194,1495583956&apos;, &apos;Hm_lpvt_0c0e9d9b1e7d617b3e6842e85b9fb068&apos;: &apos;1495583991&apos; &#125; params = [] def __init__(self): field = [&apos;标题&apos;, &apos;作者&apos;, &apos;发表时间&apos;, &apos;阅读量&apos;, &apos;评论数&apos;, &apos;点赞数&apos;, &apos;打赏数&apos;, &apos;所投专题&apos;] self.write = CSV(&apos;main.csv&apos;, field) def totalPage(self): for i in range(1,16): data = &apos;&amp;&apos;.join(self.params) url = &apos;http://www.jianshu.com/?&apos; + data + &apos;&amp;page=&#123;&#125;&apos;.format(i) self.getData(url) def getData(self,url): print url html = requests.get(url,headers = self.headers,cookies = self.cookies).text response = etree.HTML(html) ids = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li&apos;) for one in ids: one = &apos;seen_snote_ids[]=&apos; + one.xpath(&apos;@data-note-id&apos;)[0] self.params.append(one) item = &#123;&#125; flag = 0 read = re.findall(r&apos;ic-list-read&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) comment = re.findall(r&apos;ic-list-comments&quot;&gt;&lt;/i&gt; (\d+)&apos;, html) result = response.xpath(&apos;//*[@id=&quot;list-container&quot;]/ul/li/div&apos;) for one in result: item[1] = one.xpath(&apos;a/text()&apos;)[0] item[2] = one.xpath(&apos;div[1]/div/a/text()&apos;)[0] item[3] = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;)[0] item[4] = read[flag] try: item[5] = comment[flag] except: item[5] = u&apos;&apos; item[6] = one.xpath(&apos;div[2]/span/text()&apos;)[0].strip() try: item[7] = one.xpath(&apos;div[2]/span[2]/text()&apos;)[0].strip() except: item[7] = u&apos;0&apos; try: item[8] = one.xpath(&apos;div[2]/a[1]/text()&apos;)[0] except: item[8] = u&apos;&apos; flag += 1 row = [item[i] for i in range(1, 9)] self.write.writeRow(row)if __name__ == &quot;__main__&quot;: jian = Spider() jian.totalPage() 结果截图 总结现在想来，在爬取网站时，我们可以携带尽可能全的参数（俗话说，礼多人不怪），避免遇到我这个错误，scrapy版本正在写，有兴趣的可以私聊参考源码。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版万年历]]></title>
    <url>%2F2017%2F06%2F15%2F%E4%B8%87%E5%B9%B4%E5%8E%86%2F</url>
    <content type="text"><![CDATA[万年历这个题目几乎是不论学哪种编程语言必要尝试的一个小知识，综合了循环，逻辑关系判断等各编程语言的基础知识。今天我们一起用Python实现简单的万年历功能（查看某年各个月份日历和查看确定月份日历）。网上大概浏览了一部分代码，发现实现方法都是大同小异，本篇文章代码可能会略有不同，可供参考。记得之前在学C语言的时候就去写过万年历，现在已经忘得一干二净了，用Python实现万年历主要参考了两篇文章，第一篇是向右奔跑老大之前写的java版万年历,还有一篇是在网上找的C语言实现万年历,大家也可以去参考这两篇文章的方法。 难点万年历实现的难点自我感觉有以下几个： 1.计算某年一月一日为周几 2.计算某年确定月份的第一天为周几 3.整个日历格式的设定（调试是真的烦） 万年历实现1.计算某年一月一日为周几这里我看到网上大部分都是写的1990年一月一日为周一这个点进行判断计算其他年份的一月一日，或者是确定月份的第一天。还有一种是计算从1——年份-1这一段时间的总天数，然后+1除以7去计算得到该年份的一月一日或者是确定月份的第一天为周几（这个应该是一种计算问题，具体更多实现方法可以自己去查），这里我采用的是计算从1——该年的上一年的总天数+1，计算该年份的一月一日的周几。2.计算某年确定月份的第一天为周几 同样的道理，计算总天数之后加上今年已经过去的月份的总天数再加1除以7取余得到3.格式问题这个没办法，只能自己去调试，调整到一个合适的输出格式即可 文字表述可能不清楚，可以参考代码+注释 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*-coding:utf-8 -*-# @name ：Python万年历# @author ：loading_miracleclass Calendar(object): days = [31,28,31,30,31,30,31,31,30,31,30,31] def __init__(self, year=2017):#初始化默认2017年 self.year = year if self.yearDays(year) == 366: self.days[1] = 29 self.totalDays = 0 for i in range(1, self.year): self.totalDays += self.yearDays(i) #判断每年的天数 def yearDays(self,year): return 366 if (year % 4 == 0 and year % 100 != 0) or year % 400 == 0 else 365 # 查看某个月的日历 def months(self,month): totalDays = self.totalDays for i in range(1,month): totalDays+=self.days[i-1] #计算确定月份的第一天为周几 self.week = (totalDays+1) % 7 self.show(month) #查看全年日历 def wholeYear(self): # 计算确定年份的一月一日为周几 self.week = (self.totalDays+1) % 7 for i in range(1,13): self.show(i) #显示输出函数 def show(self,month): print &apos;\t\t&#123;&#125;年&#123;&#125;月份日历&apos;.format(self.year,month) print &apos;Sun Mon Tues Web Thur Fri Sat&apos; print &apos;-----------------------------------------&apos; begin = 1 for j in range(0, self.week): print &apos;%4s&apos; % &apos;&apos;, while begin &lt;= self.days[month - 1]: print &apos;%4d&apos; % begin, begin += 1 self.week = (self.week + 1) % 7 if self.week % 7 == 0: print print &apos;\n\n&apos;if __name__ == &quot;__main__&quot;: data = Calendar() data.wholeYear() data.months(5) 效果截图]]></content>
      <categories>
        <category>Python编程练习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>基础编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫—简书七日热门（scrapy）]]></title>
    <url>%2F2017%2F06%2F14%2FPython%E7%88%AC%E8%99%AB%E2%80%94%E7%AE%80%E4%B9%A6%E4%B8%83%E6%97%A5%E7%83%AD%E9%97%A8%EF%BC%88scrapy%EF%BC%89%2F</url>
    <content type="text"><![CDATA[简书七日热门的数据，主要爬取了以下几个字段： 1.用户2.标题3.阅读量4.评论量5.获赞量6.打赏数7.文章发表时间8.被哪些专题收录 首先我们可以先看一下我们要获取字段的位置 框中的其他数据都可以通过xpath获取到，标注箭头的为我在当前页面通过xpath获取不到的或者出现问题的，不过别着急，二级页面中包含我们想要的所有数据。 从这个页面我们可以看到，在第一级页面中的数据，在这一页都可以看到，但我们只选取我们想要的字段,箭头指向的为上一级页面中通过xpath获取出现问题的字段，在当前页面获取同样出现了问题，所以果断采用正则在源码中获取 我们可以看到在源码中可以看到这些字段,那么就可以通过正则来匹配相应的字段我们可以看一下这一篇文章被哪些专题收录，在文章的最下边 现在我们整明白了所有字段的位置，接下来就开始去抓取这些字段，首先我们先分析页面的分页问题，通过观察发现页面是动态加载的，再点击更多后会请求新的数据，我们来看一下整个页面的url规则，以及获取被收录专题的数据的url规则，我这里简单列一下一些截图，具体可以参考罗罗攀的Python爬虫之简书七日热门数据爬取（异步加载详解），写的特别详细。 通过这些字段我们可以想到一种构造url的方法1234def start_requests(self): for a in range(1,6): self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a yield scrapy.Request(self.url,self.parse) 我们再来看一下被收录专题的url规则 从这两张图可以看出，URL主要变化点在id和和page，ID可以从第三张图片中通过正则从源码获取从最后一张图可以看出收录的专题数有5页专题名都存在title中所以我们就可以通过第一条URL获取总页数，然后构造其他的url。构造方法如下：1234567891011121314id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0]url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %iddatas = []result = requests.get(url)data = json.loads(result.text)for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;])count = data[&apos;total_pages&apos;]for one in range(2,count+1): url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one) result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) 通过以上方法可以成功的找到页面和数据，接下来一个很重要的问题，怎们在多级页面中统一数据，因为我们所有的数据是集中在两个页面是实现的，那么怎样才可以实现多级页面传递数据呢？完成多级页面传递参数主要是通过meta这个属性，具体传递方法可以参照向右奔跑老大的Scrapy抓取在不同级别Request之间传递参数，这篇文章有很详细的介绍。在这个地方遇到的一个坑：多级页面传递后，发现总是存的是重复的内容，在这里一定要注意，因为肯定是使用for循环去获取相关标题的URL然后进入子页面去获取其他相关字段的信息，但是如果在这个地方定义了item，然后在多级传递参数，就可能出现重复的情况，所以就采用了在第一级页面获取到的数据先用变量存储，通过meta这个属性标签带入二级页面，可以看一下处理代码：12345678910111213for one in result: user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0] title = one.xpath(&apos;a/text()&apos;).extract()[0] zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0] try: shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0] except: shang = u&apos;0&apos; publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0] link = one.xpath(&apos;a/@href&apos;).extract()[0] url = urljoin_rfc(base_url,link) yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title, &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type) 通过上边的简单介绍，应该对整个流程和规则有了一定的了解，下边直接上代码 1.item.py123456789101112import scrapyclass JianshuItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() user = scrapy.Field() #用户 title = scrapy.Field() #标题 count_read = scrapy.Field() #阅读量 ping = scrapy.Field() #评论量 count_zan = scrapy.Field() #喜欢 shang = scrapy.Field() #赞赏 publish = scrapy.Field() #时间 sp_title = scrapy.Field() #被专题书录 ##2.jianshuSpider.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#-*- coding:utf-8 -*-import scrapyfrom ..items import JianshuItemfrom scrapy.utils.url import urljoin_rfcimport requestsimport reimport jsonclass jianshuSpider(scrapy.Spider): name = &apos;jianshu&apos; def start_requests(self): for a in range(1,6): self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a yield scrapy.Request(self.url,self.parse) def parse(self, response): result = response.xpath(&apos;/html/body/div[1]/div/div[1]/div/ul/li/div&apos;) base_url = &apos;http://www.jianshu.com&apos; for one in result: user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0] title = one.xpath(&apos;a/text()&apos;).extract()[0] zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0] try: shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0] except: shang = u&apos;0&apos; publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0] link = one.xpath(&apos;a/@href&apos;).extract()[0] url = urljoin_rfc(base_url,link) yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title, &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type) def parse_type(self,response): item = JianshuItem() item[&apos;user&apos;] = response.meta[&apos;user&apos;] item[&apos;title&apos;] = response.meta[&apos;title&apos;] item[&apos;count_read&apos;] = re.findall(&apos;&quot;views_count&quot;:(.*?),&apos;,response.text,re.S)[0] item[&apos;ping&apos;] = re.findall(&apos;&quot;comments_count&quot;:(.*?),&apos;,response.text,re.S)[0] item[&apos;count_zan&apos;] = response.meta[&apos;zan&apos;] item[&apos;shang&apos;] = response.meta[&apos;shang&apos;] item[&apos;publish&apos;] = response.meta[&apos;publish&apos;] id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0] url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %id datas = [] result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) count = data[&apos;total_pages&apos;] for one in range(2,count+1): url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one) result = requests.get(url) data = json.loads(result.text) for one in data[&apos;collections&apos;]: datas.append(one[&apos;title&apos;]) try: item[&apos;sp_title&apos;] = &quot; &quot;.join(datas).encode(&apos;utf-8&apos;) except: item[&apos;sp_title&apos;] = u&apos;&apos; yield item 3.结果 总结先说一下存在的问题：在对一个页面进行爬取的时候没做一个整体的考虑，所以在不同页面之前传递的参数比较多，代码看起来有点乱，之后可以将所有的数据全部在二级页面获取。还有一点，可以看到在处理json数据那一块，我选择使用了requests库去操作，这是因为如果使用scrapy.Request去操作的话，要通过回调函数去处理，还要在进行参数的传递，造成很多麻烦，然后准备尝试去写一个用requests和beautifulsoup爬虫，进行一个对比。写的第一篇关于思想和技术方面的文章，由于时间的关系所以可能解释的不是很详细，如果存在问题可以在下方评论一起探讨。最后还是要感谢一下罗罗攀和向右奔跑的相关文章和罗罗攀大哥无私的帮助，如果有时间的话会对数据进一步处理，做一个相关分析。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F06%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>