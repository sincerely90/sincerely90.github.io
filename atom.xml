<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Loading_miracle</title>
  <subtitle>Liang</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://sincerely90.github.io/"/>
  <updated>2017-06-15T06:00:51.456Z</updated>
  <id>https://sincerely90.github.io/</id>
  
  <author>
    <name>Liang</name>
    <email>fengshengjie5@live.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python爬虫—简书七日热门（scrapy）</title>
    <link href="https://sincerely90.github.io/2017/06/14/Python%E7%88%AC%E8%99%AB%E2%80%94%E7%AE%80%E4%B9%A6%E4%B8%83%E6%97%A5%E7%83%AD%E9%97%A8%EF%BC%88scrapy%EF%BC%89/"/>
    <id>https://sincerely90.github.io/2017/06/14/Python爬虫—简书七日热门（scrapy）/</id>
    <published>2017-06-14T08:37:07.605Z</published>
    <updated>2017-06-15T06:00:51.456Z</updated>
    
    <content type="html"><![CDATA[<p>简书七日热门的数据，主要爬取了以下几个字段：</p>
<ul>
<li>1.用户<br>2.标题<br>3.阅读量<br>4.评论量<br>5.获赞量<br>6.打赏数<br>7.文章发表时间<br>8.被哪些专题收录</li>
</ul>
<p><strong> 首先我们可以先看一下我们要获取字段的位置 </strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-09a0c9d0e262d802.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><em>框中的其他数据都可以通过xpath获取到，标注箭头的为我在当前页面通过xpath获取不到的或者出现问题的，不过别着急，二级页面中包含我们想要的所有数据。</em></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-3148cf54be37eb91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><em> 从这个页面我们可以看到，在第一级页面中的数据，在这一页都可以看到，但我们只选取我们想要的字段,箭头指向的为上一级页面中通过xpath获取出现问题的字段，在当前页面获取同样出现了问题，所以果断采用正则在源码中获取</em></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-20e51a326b6ee30d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><em>我们可以看到在源码中可以看到这些字段,那么就可以通过正则来匹配相应的字段</em><br><strong>我们可以看一下这一篇文章被哪些专题收录，在文章的最下边</strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-200b2150db16cfdc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>现在我们整明白了所有字段的位置，接下来就开始去抓取这些字段，首先我们先分析页面的分页问题，通过观察发现页面是动态加载的，再点击更多后会请求新的数据，我们来看一下整个页面的url规则，以及获取被收录专题的数据的url规则，我这里简单列一下一些截图，具体可以参考<a href="http://www.jianshu.com/u/9104ebf5e177" target="_blank" rel="external">罗罗攀</a>的<a href="http://www.jianshu.com/p/da54149a0944" target="_blank" rel="external">Python爬虫之简书七日热门数据爬取（异步加载详解）</a>，写的特别详细。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-76eb0953388caff9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-7d512c0ff24bc711.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><em>通过这些字段我们可以想到一种构造url的方法</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def start_requests(self):</div><div class="line">        for a in range(1,6):</div><div class="line">            self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a</div><div class="line">            yield scrapy.Request(self.url,self.parse)</div></pre></td></tr></table></figure></p>
<p>我们再来看一下被收录专题的url规则</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-24c6254d3a3973ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-f24e40813e608920.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-95a9795f83a0df1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/5208064-7f46dc36aef1ca7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><em>从这两张图可以看出，URL主要变化点在id和和page，<strong>ID</strong>可以从第三张图片中通过正则从源码获取从最后一张图可以看出收录的专题数有<strong>5页</strong>专题名都存在<strong>title</strong>中所以我们就可以通过第一条URL获取总页数，然后构造其他的url。<br>构造方法如下：</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0]</div><div class="line">url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %id</div><div class="line">datas = []</div><div class="line">result = requests.get(url)</div><div class="line">data = json.loads(result.text)</div><div class="line">for one in data[&apos;collections&apos;]:</div><div class="line">    datas.append(one[&apos;title&apos;])</div><div class="line">count = data[&apos;total_pages&apos;]</div><div class="line">for one in range(2,count+1):</div><div class="line">    url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one)</div><div class="line">    result = requests.get(url)</div><div class="line">    data = json.loads(result.text)</div><div class="line">    for one in data[&apos;collections&apos;]:</div><div class="line">        datas.append(one[&apos;title&apos;])</div></pre></td></tr></table></figure></p>
<p><strong>通过以上方法可以成功的找到页面和数据，接下来一个很重要的问题，怎们在多级页面中统一数据，因为我们所有的数据是集中在两个页面是实现的，那么怎样才可以实现多级页面传递数据呢？</strong><br>完成多级页面传递参数主要是通过<strong>meta</strong>这个属性，具体传递方法可以参照<a href="http://www.jianshu.com/u/54b5900965ea" target="_blank" rel="external">向右奔跑</a>老大的<a href="http://www.jianshu.com/p/de61ed0f961d" target="_blank" rel="external">Scrapy抓取在不同级别Request之间传递参数</a>，这篇文章有很详细的介绍。<br>在这个地方遇到的一个坑：多级页面传递后，发现总是存的是重复的内容，在这里一定要注意，因为肯定是使用for循环去获取相关标题的URL然后进入子页面去获取其他相关字段的信息，但是如果在这个地方定义了item，然后在多级传递参数，就可能出现重复的情况，所以就采用了在第一级页面获取到的数据先用变量存储，通过meta这个属性标签带入二级页面，可以看一下处理代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">for one in result:</div><div class="line">    user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0]</div><div class="line">    title = one.xpath(&apos;a/text()&apos;).extract()[0]</div><div class="line">    zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0]</div><div class="line">    try:</div><div class="line">        shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0]</div><div class="line">    except:</div><div class="line">        shang = u&apos;0&apos;</div><div class="line">    publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0]</div><div class="line">    link = one.xpath(&apos;a/@href&apos;).extract()[0]</div><div class="line">    url = urljoin_rfc(base_url,link)</div><div class="line">    yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title,</div><div class="line">                                        &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type)</div></pre></td></tr></table></figure></p>
<p><strong>通过上边的简单介绍，应该对整个流程和规则有了一定的了解，下边直接上代码</strong></p>
<h2 id="1-item-py"><a href="#1-item-py" class="headerlink" title="1.item.py"></a>1.item.py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line">class JianshuItem(scrapy.Item):</div><div class="line">    # define the fields for your item here like:</div><div class="line">    # name = scrapy.Field()</div><div class="line">    user = scrapy.Field() #用户</div><div class="line">    title = scrapy.Field() #标题</div><div class="line">    count_read = scrapy.Field() #阅读量</div><div class="line">    ping = scrapy.Field() #评论量</div><div class="line">    count_zan = scrapy.Field() #喜欢</div><div class="line">    shang = scrapy.Field()  #赞赏</div><div class="line">    publish = scrapy.Field() #时间</div><div class="line">    sp_title = scrapy.Field() #被专题书录</div></pre></td></tr></table></figure>
<p>##2.jianshuSpider.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line">#-*- coding:utf-8 -*-</div><div class="line">import scrapy</div><div class="line">from ..items import JianshuItem</div><div class="line">from scrapy.utils.url import urljoin_rfc</div><div class="line">import requests</div><div class="line">import re</div><div class="line">import json</div><div class="line">class jianshuSpider(scrapy.Spider):</div><div class="line">    name = &apos;jianshu&apos;</div><div class="line">    def start_requests(self):</div><div class="line">        for a in range(1,6):</div><div class="line">            self.url = &apos;http://www.jianshu.com/trending/weekly?&amp;page=%s&apos; %a</div><div class="line">            yield scrapy.Request(self.url,self.parse)</div><div class="line">    def parse(self, response):</div><div class="line">        result = response.xpath(&apos;/html/body/div[1]/div/div[1]/div/ul/li/div&apos;)</div><div class="line">        base_url = &apos;http://www.jianshu.com&apos;</div><div class="line">        for one in result:</div><div class="line">            user = one.xpath(&apos;div[1]/div/a/text()&apos;).extract()[0]</div><div class="line">            title = one.xpath(&apos;a/text()&apos;).extract()[0]</div><div class="line">            zan = one.xpath(&apos;div[2]/span[1]/text()&apos;).extract()[0]</div><div class="line">            try:</div><div class="line">                shang = one.xpath(&apos;div[2]/span[2]/text()&apos;).extract()[0]</div><div class="line">            except:</div><div class="line">                shang = u&apos;0&apos;</div><div class="line">            publish = one.xpath(&apos;div[1]/div/span/@data-shared-at&apos;).extract()[0]</div><div class="line">            link = one.xpath(&apos;a/@href&apos;).extract()[0]</div><div class="line">            url = urljoin_rfc(base_url,link)</div><div class="line">            yield scrapy.Request(url,meta=&#123;&apos;user&apos;:user,&apos;title&apos;:title,</div><div class="line">                                                &apos;zan&apos;:zan,&apos;shang&apos;:shang,&apos;publish&apos;:publish&#125;,callback=self.parse_type)</div><div class="line">    def parse_type(self,response):</div><div class="line">        item = JianshuItem()</div><div class="line">        item[&apos;user&apos;] = response.meta[&apos;user&apos;]</div><div class="line">        item[&apos;title&apos;] = response.meta[&apos;title&apos;]</div><div class="line">        item[&apos;count_read&apos;] = re.findall(&apos;&quot;views_count&quot;:(.*?),&apos;,response.text,re.S)[0]</div><div class="line">        item[&apos;ping&apos;] = re.findall(&apos;&quot;comments_count&quot;:(.*?),&apos;,response.text,re.S)[0]</div><div class="line">        item[&apos;count_zan&apos;] = response.meta[&apos;zan&apos;]</div><div class="line">        item[&apos;shang&apos;] = response.meta[&apos;shang&apos;]</div><div class="line">        item[&apos;publish&apos;] = response.meta[&apos;publish&apos;]</div><div class="line">        id = re.findall(&apos;&#123;&quot;id&quot;:(.*?),&apos;, response.text, re.S)[0]</div><div class="line">        url = &apos;http://www.jianshu.com/notes/%s/included_collections?page=1&apos; %id</div><div class="line">        datas = []</div><div class="line">        result = requests.get(url)</div><div class="line">        data = json.loads(result.text)</div><div class="line">        for one in data[&apos;collections&apos;]:</div><div class="line">            datas.append(one[&apos;title&apos;])</div><div class="line">        count = data[&apos;total_pages&apos;]</div><div class="line">        for one in range(2,count+1):</div><div class="line">            url = &apos;http://www.jianshu.com/notes/&#123;&#125;/included_collections?page=&#123;&#125;&apos;.format(id,one)</div><div class="line">            result = requests.get(url)</div><div class="line">            data = json.loads(result.text)</div><div class="line">            for one in data[&apos;collections&apos;]:</div><div class="line">                datas.append(one[&apos;title&apos;])</div><div class="line">        try:</div><div class="line">            item[&apos;sp_title&apos;] = &quot; &quot;.join(datas).encode(&apos;utf-8&apos;)</div><div class="line">        except:</div><div class="line">            item[&apos;sp_title&apos;] = u&apos;&apos;</div><div class="line">        yield item</div></pre></td></tr></table></figure></p>
<h2 id="3-结果"><a href="#3-结果" class="headerlink" title="3.结果"></a>3.结果</h2><p><img src="http://upload-images.jianshu.io/upload_images/5208064-b3efbae7e3b18775.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>先说一下存在的问题：<br>在对一个页面进行爬取的时候没做一个整体的考虑，所以在不同页面之前传递的参数比较多，代码看起来有点乱，之后可以将所有的数据全部在二级页面获取。<br>还有一点，可以看到在处理json数据那一块，我选择使用了requests库去操作，这是因为如果使用scrapy.Request去操作的话，要通过回调函数去处理，还要在进行参数的传递，造成很多麻烦，然后准备尝试去写一个用requests和beautifulsoup爬虫，进行一个对比。<br><strong>写的第一篇关于思想和技术方面的文章，由于时间的关系所以可能解释的不是很详细，如果存在问题可以在下方评论一起探讨。<br>最后还是要感谢一下<a href="http://www.jianshu.com/u/9104ebf5e177" target="_blank" rel="external">罗罗攀</a>和<a href="http://www.jianshu.com/u/54b5900965ea" target="_blank" rel="external">向右奔跑</a>的相关文章和<a href="http://www.jianshu.com/u/9104ebf5e177" target="_blank" rel="external">罗罗攀</a>大哥无私的帮助，如果有时间的话会对数据进一步处理，做一个相关分析。</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简书七日热门的数据，主要爬取了以下几个字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.用户&lt;br&gt;2.标题&lt;br&gt;3.阅读量&lt;br&gt;4.评论量&lt;br&gt;5.获赞量&lt;br&gt;6.打赏数&lt;br&gt;7.文章发表时间&lt;br&gt;8.被哪些专题收录&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt; 首先我们
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://sincerely90.github.io/2017/06/13/hello-world/"/>
    <id>https://sincerely90.github.io/2017/06/13/hello-world/</id>
    <published>2017-06-13T03:06:24.221Z</published>
    <updated>2017-06-13T03:06:24.221Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
